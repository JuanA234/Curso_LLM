{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e38a807-cc10-443a-91bc-7cac62526a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_ANTHROPIC = 'claude-3-haiku-20240307'\n",
    "MODEL_GEMINI = 'gemini-2.0-flash-exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880c14ef-e435-45dc-a35f-17bdb9621b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyDZ\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93fe9a1-e120-4f0b-ab4c-6404cc5b0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcc424b-7d70-45c0-b10f-50338d5d8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt del sistema\n",
    "system_prompt = \"Eres un experto en LLM, inteligencia artificial y python \\\n",
    "el cual se le van a proporcionar una o varias preguntas técnicas relacionadas a los temas a los que eres experto \\\n",
    "y tienes que responder de tal manera que un estudiante que es muy novato y apenas está aprendiendo entienda\"\n",
    "system_prompt += \"Siempre se preciso. si no sabes la respuesto, dilo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4a73e8-e9c4-444c-8332-020e8f3dcbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "question = \"\"\"\n",
    "Por favor explica que hace este código y porque:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ded27f-7b11-4aee-8d1f-2949e00f8591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL_GPT, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208d0018-25df-400f-8c38-f01b7e6d8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1796891-61c0-4309-ba51-721975086fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def artist(city):\n",
    "    image_response = openai.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=f\"\",\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\",\n",
    "        )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2779b7-f108-406b-8566-9e2a4f6235a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = artist(\"New York City\")\n",
    "#display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d578d3a-aaa2-4b89-acce-adf24c64aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "def play_audio(audio_segment):\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    temp_path = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "    try:\n",
    "        audio_segment.export(temp_path, format=\"wav\")\n",
    "        time.sleep(3) # Student Dominic found that this was needed. You could also try commenting out to see if not needed on your PC\n",
    "        subprocess.call([\n",
    "            \"ffplay\",\n",
    "            \"-nodisp\",\n",
    "            \"-autoexit\",\n",
    "            \"-hide_banner\",\n",
    "            temp_path\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(temp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    " \n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",  # Also, try replacing onyx with alloy\n",
    "        input=message\n",
    "    )\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play_audio(audio)\n",
    "\n",
    "talker(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a7d839-8b86-423e-8c54-b49c7aa6f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_gpt(history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        #stream=True\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return result\n",
    "    #for chunk in stream:\n",
    "        #result += chunk.choices[0].delta.content or \"\"\n",
    "        #yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4cfb3d-aa87-43d1-969b-6e2f02005102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_claude(history):\n",
    "\n",
    "    cleaned_history = [\n",
    "    {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "    for msg in history\n",
    "    ]\n",
    "    result = claude.messages.create(\n",
    "        model=MODEL_ANTHROPIC,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_prompt,\n",
    "        messages=cleaned_history,\n",
    "    )\n",
    "\n",
    "    response = result.content[0].text\n",
    "    return response\n",
    "    #response = \"\"\n",
    "    #with result as stream:\n",
    "        #for text in stream.text_stream:\n",
    "            #response += text or \"\"\n",
    "            #yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d278951-24ce-4db9-a30c-9c065a96c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_llama(history):\n",
    "    messages = [{\"role\":\"system\", \"content\": system_prompt}] + history\n",
    "    result = ollama.chat(\n",
    "        model = MODEL_LLAMA,\n",
    "        messages = messages,\n",
    "        #stream = True\n",
    "    )\n",
    "    return result['message']['content']\n",
    "    #display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    #for chunk in stream:\n",
    "        #response += chunk['message']['content'] or ''\n",
    "        #response = response.replace(\"```\", \"\").replace(\"markdown\",\"\")\n",
    "        #update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd0331f3-c3f5-4dc3-b5fc-4c326d4cfac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_model(history, model):\n",
    "    result = ''\n",
    "    if model==\"GPT\":\n",
    "        result = response_gpt(history)\n",
    "    elif model==\"Claude\":\n",
    "        result = response_claude(history)\n",
    "    elif model==\"Llama\":\n",
    "        result = response_llama(history)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    return result\n",
    "    #yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29b72509-a228-4074-bdfb-19bfc110e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history, model):\n",
    "    image = None\n",
    "        \n",
    "    reply = response_model(history, model)\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "\n",
    "    # Comment out or delete the next line if you'd rather skip Audio for now..\n",
    "    talker(reply)\n",
    "    \n",
    "    return history, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de7e07f7-46d2-498b-aab8-9015b523e45c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_output = gr.Image(height=500)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\" , scale=3)\n",
    "        model = gr.Dropdown(choices = [\"GPT\", \"Claude\", \"Llama\"], label=\"Select model\", scale=1)\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def do_entry(message, history, selection):\n",
    "        history += [{\"role\": \"user\", \"content\": f\"{message} \"}]\n",
    "        return \"\", history, selection\n",
    "\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot, model], outputs=[entry, chatbot, model]).then(\n",
    "        chat, inputs=[chatbot, model], outputs=[chatbot, image_output]\n",
    "    )\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4abade-d185-45d4-a70c-0887dde11735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
