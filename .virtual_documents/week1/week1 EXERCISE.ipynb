


# imports
import os
from dotenv import load_dotenv
from IPython.display import Markdown, display, update_display
from openai import OpenAI


# constants

MODEL_GPT = 'gpt-4o-mini'
MODEL_LLAMA = 'llama3.2'


# set up environment
load_dotenv(override=True)
api_key = os.getenv('OPENAI_API_KEY')
if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:
    print("API key se ve muy bien")
else:
    print("There might be a problem with your API key? Please visit the troubleshooting notebook!")
openai = OpenAI()


#Prompt del sistema
system_prompt = "Eres un experto en LLM, inteligencia artificial y python \
el cual se le van a proporcionar una o varias preguntas técnicas relacionadas a los temas a los que eres experto \
y tienes que responder de tal manera que un estudiante que es muy novato y apenas está aprendiendo entienda"


# here is the question; type over this to ask something new

question = """
Por favor explica que hace este código y porque:
yield from {book.get("author") for book in books if book.get("author")}
"""


# Prompt del usuario
user_prompt = f"Responde la siguiente pregunta en formato markdown: {question}"


# Crear la repuesta sin streaming
def crear_respuesta():
    response = openai.chat.completions.create(
        model = MODEL_GPT,
        messages = [
            {"role":"system", "content": system_prompt},
            {"role":"user", "content": user_prompt}
        ],
    )
    result = response.choices[0].message.content
    display(Markdown(result))


crear_respuesta()


# Get gpt-4o-mini to answer, with streaming
def stream_respuesta():
    stream = openai.chat.completions.create(
        model = MODEL_GPT,
        messages = [
            {"role":"system", "content": system_prompt},
            {"role":"user", "content": user_prompt}
        ],
        stream = True
    )
    response = ""
    display_handle = display(Markdown(""), display_id=True)
    for chunk in stream:
        response += chunk.choices[0].delta.content or ''
        #response = response.replace("```", "").replace("markdown","")
        update_display(Markdown(response), display_id=display_handle.display_id)


stream_respuesta()


# Get Llama 3.2 to answer
import ollama

def stream_respuesta_ollama():
    stream = ollama.chat(
        model = MODEL_LLAMA,
        messages = [
            {"role":"system", "content": system_prompt},
            {"role":"user", "content": user_prompt}
        ],
        stream = True
    )
    response = ""
    display_handle = display(Markdown(""), display_id=True)
    for chunk in stream:
        response += chunk['message']['content'] or ''
        #response = response.replace("```", "").replace("markdown","")
        update_display(Markdown(response), display_id=display_handle.display_id)


stream_respuesta_ollama()
